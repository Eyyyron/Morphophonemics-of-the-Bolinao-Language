{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30d17b04",
      "metadata": {
        "id": "30d17b04"
      },
      "source": [
        "# (Root → Surface) — Bolinao pronoun changes\n",
        "\n",
        "This notebook does the **forward** direction of the constrained morphophonemic work in the pronoun notebook:\n",
        "\n",
        "- **Input**: an underlying/root form (e.g., `iti`, `sai`, `ko`), or an **explicit token sequence** (e.g., `mo + ya`, `koN + ko`).\n",
        "- **Output**: one or more **candidate surface forms** with an explicit **rule trace** and weights.\n",
        "\n",
        "Scope / constraints (matches the analyzer philosophy):\n",
        "\n",
        "- This is a **constrained surface generator**, not a full phonology engine.\n",
        "- Lexicalized / honorific contractions (e.g., `mo + ya → ma`) are treated as **stored outcomes**.\n",
        "- Some transformations are **optional / semi-reversible**, so we may output multiple candidates (including an identity fallback)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38ec924f",
      "metadata": {
        "id": "38ec924f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    return (s or \"\").strip().lower()\n",
        "\n",
        "\n",
        "# Optional: load the lexicon for quick lookup of generated candidates.\n",
        "LEXICON_PATH = \"Bolinao Lexicon - bolinao_lexicon_final.csv\"\n",
        "try:\n",
        "    bolinao_final = pd.read_csv(LEXICON_PATH)\n",
        "except Exception:\n",
        "    bolinao_final = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c4ce599",
      "metadata": {
        "id": "6c4ce599"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class Candidate:\n",
        "    surface: str\n",
        "    underlying_tokens: Tuple[str, ...]\n",
        "    pos: str\n",
        "    rule: str\n",
        "    rule_type: str\n",
        "    reversibility: str\n",
        "    weight: float\n",
        "    notes: str = \"\"\n",
        "\n",
        "\n",
        "def _parse_underlying(user_input: str) -> Tuple[str, ...]:\n",
        "    \"\"\"\n",
        "    Accepts either a single form (\"iti\") or a token sequence (\"mo + ya\").\n",
        "    \"\"\"\n",
        "    s = _norm(user_input)\n",
        "    if \"+\" in s:\n",
        "        toks = [t.strip() for t in s.split(\"+\") if t.strip()]\n",
        "        return tuple(toks)\n",
        "    return (s,)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Forward (underlying/root → surface) rules\n",
        "# These are the forward counterparts of the rules in the pronoun analyzer notebook.\n",
        "# -------------------------------------------------------------------\n",
        "_LEXICALIZED_FORWARD: Dict[Tuple[str, ...], Tuple[str, float, str]] = {\n",
        "    (\"mo\", \"ya\"): (\"ma\", 0.98, \"Lexicalized/honorific contraction: mo + ya → ma (stored outcome).\"),\n",
        "    (\"ko\", \"ka\"): (\"ta\", 0.98, \"Lexicalized/honorific contraction: ko + ka → ta (stored outcome).\"),\n",
        "    (\"ko\", \"ya\"): (\"kwa\", 0.98, \"Lexicalized/honorific contraction: ko + ya → kwa (stored outcome).\"),\n",
        "    (\"mo\", \"ko\"): (\"nako\", 0.98, \"Lexicalized/honorific contraction: mo + ko → nako (stored outcome).\"),\n",
        "}\n",
        "\n",
        "# Demonstrative clitic/reduction surfaces (inverse of demo_map in analyzer).\n",
        "_DPR_UNDERLYING_TO_SURFACES: Dict[str, List[Tuple[str, float, str]]] = {\n",
        "    \"iti\": [(\"moyti\", 0.60, \"DPr clitic/reduction variant (semi-reversible).\"), (\"modti\", 0.60, \"DPr clitic/reduction variant (semi-reversible).\")],\n",
        "    \"in\": [(\"moin\", 0.60, \"DPr clitic/reduction variant (semi-reversible).\")],\n",
        "    \"isen\": [(\"modsen\", 0.60, \"DPr clitic/reduction variant (semi-reversible).\")],\n",
        "    \"taw\": [(\"moytaw\", 0.60, \"DPr clitic/reduction variant (semi-reversible).\")],\n",
        "    \"itaw\": [(\"modtaw\", 0.60, \"DPr clitic/reduction variant (semi-reversible).\")],\n",
        "}\n",
        "\n",
        "# koN-/ikon- supportive surfaces (inverse of kon_underlying in analyzer).\n",
        "_KON_UNDERLYING_TO_SURFACE: Dict[Tuple[str, ...], Tuple[str, float, str]] = {\n",
        "    (\"kon\", \"ko\"): (\"kongko\", 0.85, \"koN-/kon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"ikon\", \"ko\"): (\"ikongko\", 0.85, \"ikon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"kon\", \"ta\"): (\"konta\", 0.85, \"koN-/kon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"ikon\", \"ta\"): (\"ikonta\", 0.85, \"ikon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"kon\", \"tamo\"): (\"kontamo\", 0.85, \"koN-/kon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"ikon\", \"tamo\"): (\"ikontamo\", 0.85, \"ikon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"kon\", \"mi\"): (\"komi\", 0.85, \"koN-/kon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"ikon\", \"mi\"): (\"ikomi\", 0.85, \"ikon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"kon\", \"mo\"): (\"komo\", 0.85, \"koN-/kon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"ikon\", \"mo\"): (\"ikomo\", 0.85, \"ikon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"kon\", \"moyo\"): (\"komoyo\", 0.85, \"koN-/kon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"ikon\", \"moyo\"): (\"ikomoyo\", 0.85, \"ikon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"kon\", \"na\"): (\"kona\", 0.85, \"koN-/kon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"ikon\", \"na\"): (\"ikona\", 0.85, \"ikon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"kon\", \"ra\"): (\"konra\", 0.85, \"koN-/kon- supportive pronoun outcome (semi-reversible).\"),\n",
        "    (\"ikon\", \"ra\"): (\"ikonra\", 0.85, \"ikon- supportive pronoun outcome (semi-reversible).\"),\n",
        "}\n",
        "\n",
        "\n",
        "def generate_surface_candidates(\n",
        "    underlying_input: str,\n",
        "    pos: Optional[str] = None,\n",
        "    *,\n",
        "    allow_identity: bool = True,\n",
        "    allow_intrg_an_prefix: bool = True,\n",
        "    allow_dpr_variants: bool = True,\n",
        "    allow_dpr_linker_suffix: bool = False,\n",
        "    allow_productive_kon_prefixing: bool = False,\n",
        "    max_results: int = 25,\n",
        "    _pos_forced: bool = False,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Forward generator that mirrors the rule inventory used in the pronoun analyzer notebook.\n",
        "\n",
        "    If `pos` is None or \"auto\", this function will generate across the relevant POS categories\n",
        "    (intrg pron / DPr / indfpro / expl) while keeping the rule set constrained.\n",
        "    \"\"\"\n",
        "    pos_in = (pos or \"\").strip()\n",
        "    pos_norm = pos_in.lower()\n",
        "    if pos_norm in (\"\", \"auto\", \"any\", \"all\"):\n",
        "        # Generate across the POS values used in the analyzer notebook.\n",
        "        all_pos = [\"intrg pron\", \"DPr\", \"indfpro\", \"expl\"]\n",
        "        combined: List[Dict[str, Any]] = []\n",
        "        for p in all_pos:\n",
        "            combined.extend(\n",
        "                generate_surface_candidates(\n",
        "                    underlying_input,\n",
        "                    p,\n",
        "                    allow_identity=allow_identity,\n",
        "                    allow_intrg_an_prefix=allow_intrg_an_prefix,\n",
        "                    allow_dpr_variants=allow_dpr_variants,\n",
        "                    allow_dpr_linker_suffix=allow_dpr_linker_suffix,\n",
        "                    allow_productive_kon_prefixing=allow_productive_kon_prefixing,\n",
        "                    max_results=max_results,\n",
        "                    _pos_forced=True,\n",
        "                )\n",
        "            )\n",
        "        # Deduplicate across POS runs (surface+underlying+rule); keep best weight.\n",
        "        best: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n",
        "        for d in combined:\n",
        "            k = (d[\"surface\"], d[\"underlying\"], d[\"rule\"])\n",
        "            if k not in best or float(d[\"weight\"]) > float(best[k][\"weight\"]):\n",
        "                best[k] = d\n",
        "        out = list(best.values())\n",
        "        out.sort(key=lambda d: float(d[\"weight\"]), reverse=True)\n",
        "        return out[:max_results]\n",
        "\n",
        "    # POS-specific generation below\n",
        "    pos = pos_in\n",
        "    underlying_tokens = _parse_underlying(underlying_input)\n",
        "    candidates: List[Candidate] = []\n",
        "\n",
        "    # 1) Lexicalized/honorific contractions: only fire when the underlying is explicitly a sequence.\n",
        "    if underlying_tokens in _LEXICALIZED_FORWARD:\n",
        "        surface, w, note = _LEXICALIZED_FORWARD[underlying_tokens]\n",
        "        candidates.append(\n",
        "            Candidate(\n",
        "                surface=surface,\n",
        "                underlying_tokens=underlying_tokens,\n",
        "                pos=pos,\n",
        "                rule=f\"LEX_CONTRACTION_{surface}\",\n",
        "                rule_type=\"lexicalized\",\n",
        "                reversibility=\"non-reversible\",\n",
        "                weight=w,\n",
        "                notes=note,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # 2) koN-/ikon- supportive forms: can fire on explicit (koN + X)/(ikon + X).\n",
        "    if underlying_tokens in _KON_UNDERLYING_TO_SURFACE:\n",
        "        surface, w, note = _KON_UNDERLYING_TO_SURFACE[underlying_tokens]\n",
        "        candidates.append(\n",
        "            Candidate(\n",
        "                surface=surface,\n",
        "                underlying_tokens=underlying_tokens,\n",
        "                pos=pos,\n",
        "                rule=f\"KON_supportive_{surface}\",\n",
        "                rule_type=\"morphophonemic\",\n",
        "                reversibility=\"semi-reversible\",\n",
        "                weight=w,\n",
        "                notes=note,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # 3) POS-specific forward generation when input is a single root token.\n",
        "    if len(underlying_tokens) == 1:\n",
        "        base = underlying_tokens[0]\n",
        "\n",
        "        # Interrogatives: optional an- prefix (forward counterpart of INTRG_an_prefix_optional_add).\n",
        "        if allow_intrg_an_prefix and pos == \"intrg pron\":\n",
        "            if not base.startswith(\"an\"):\n",
        "                candidates.append(\n",
        "                    Candidate(\n",
        "                        surface=\"an\" + base,\n",
        "                        underlying_tokens=underlying_tokens,\n",
        "                        pos=pos,\n",
        "                        rule=\"INTRG_an_prefix_optional_add\",\n",
        "                        rule_type=\"morphophonemic\",\n",
        "                        reversibility=\"semi-reversible\",\n",
        "                        weight=0.55,\n",
        "                        notes=\"Optional/historical an- prefix (semi-reversible).\",\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        # Demonstratives: reduced/cliticized variants (inverse of DPR_reduction_*).\n",
        "        if allow_dpr_variants and pos == \"DPr\":\n",
        "            for surface, w, note in _DPR_UNDERLYING_TO_SURFACES.get(base, []):\n",
        "                candidates.append(\n",
        "                    Candidate(\n",
        "                        surface=surface,\n",
        "                        underlying_tokens=underlying_tokens,\n",
        "                        pos=pos,\n",
        "                        rule=f\"DPR_reduction_{surface}\",\n",
        "                        rule_type=\"morphophonemic\",\n",
        "                        reversibility=\"semi-reversible\",\n",
        "                        weight=w,\n",
        "                        notes=note,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # Conservative linker suffix generation (forward counterpart to DPR_linker_y/n).\n",
        "            if allow_dpr_linker_suffix and len(base) > 1:\n",
        "                candidates.append(\n",
        "                    Candidate(\n",
        "                        surface=base + \"y\",\n",
        "                        underlying_tokens=underlying_tokens,\n",
        "                        pos=pos,\n",
        "                        rule=\"DPR_linker_y\",\n",
        "                        rule_type=\"morphophonemic\",\n",
        "                        reversibility=\"semi-reversible\",\n",
        "                        weight=0.35,\n",
        "                        notes=\"Conservative: treat final -y as linker/clitic.\",\n",
        "                    )\n",
        "                )\n",
        "                candidates.append(\n",
        "                    Candidate(\n",
        "                        surface=base + \"n\",\n",
        "                        underlying_tokens=underlying_tokens,\n",
        "                        pos=pos,\n",
        "                        rule=\"DPR_linker_n\",\n",
        "                        rule_type=\"morphophonemic\",\n",
        "                        reversibility=\"semi-reversible\",\n",
        "                        weight=0.35,\n",
        "                        notes=\"Conservative: treat final -n as linker/clitic.\",\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        # Optional: productive koN-/ikon- prefixing from a basic pronoun token.\n",
        "        if allow_productive_kon_prefixing:\n",
        "            for pref in (\"kon\", \"ikon\"):\n",
        "                key = (pref, base)\n",
        "                if key in _KON_UNDERLYING_TO_SURFACE:\n",
        "                    surface, w, note = _KON_UNDERLYING_TO_SURFACE[key]\n",
        "                    candidates.append(\n",
        "                        Candidate(\n",
        "                            surface=surface,\n",
        "                            underlying_tokens=(pref, base),\n",
        "                            pos=pos,\n",
        "                            rule=f\"KON_supportive_{surface}\",\n",
        "                            rule_type=\"morphophonemic\",\n",
        "                            reversibility=\"semi-reversible\",\n",
        "                            weight=min(0.50, w),\n",
        "                            notes=note + \" (generated assuming optional supportive prefixing)\",\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "    # 4) Identity candidate (always safe; does not claim derivation).\n",
        "    # In auto mode we still want identity; in POS-forced runs, identity would duplicate a lot; keep it but dedupe later.\n",
        "    if allow_identity and (not _pos_forced or pos in (\"indfpro\", \"expl\")):\n",
        "        surface_identity = \" + \".join(underlying_tokens) if len(underlying_tokens) > 1 else underlying_tokens[0]\n",
        "        candidates.append(\n",
        "            Candidate(\n",
        "                surface=surface_identity,\n",
        "                underlying_tokens=underlying_tokens,\n",
        "                pos=pos,\n",
        "                rule=\"LEX_FALLBACK_identity\",\n",
        "                rule_type=\"lexicalized\",\n",
        "                reversibility=\"non-reversible\",\n",
        "                weight=0.10,\n",
        "                notes=\"Identity fallback: keep underlying as surface (no derivation claim).\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Deduplicate by (surface, underlying_tokens, rule).\n",
        "    seen = set()\n",
        "    uniq: List[Candidate] = []\n",
        "    for c in candidates:\n",
        "        key = (c.surface, c.underlying_tokens, c.rule)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        uniq.append(c)\n",
        "\n",
        "    uniq.sort(key=lambda c: c.weight, reverse=True)\n",
        "    uniq = uniq[:max_results]\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            \"surface\": c.surface,\n",
        "            \"underlying\": \" + \".join(c.underlying_tokens),\n",
        "            \"pos\": c.pos,\n",
        "            \"rule\": c.rule,\n",
        "            \"rule_type\": c.rule_type,\n",
        "            \"reversibility\": c.reversibility,\n",
        "            \"weight\": float(c.weight),\n",
        "            \"notes\": c.notes,\n",
        "        }\n",
        "        for c in uniq\n",
        "    ]\n",
        "\n",
        "\n",
        "def lookup_in_lexicon(surface_forms: Sequence[str]) -> Optional[pd.DataFrame]:\n",
        "    if bolinao_final is None:\n",
        "        return None\n",
        "    if \"word\" not in bolinao_final.columns:\n",
        "        return None\n",
        "    forms = {_norm(s) for s in surface_forms}\n",
        "    df = bolinao_final.copy()\n",
        "    df[\"_word_norm\"] = df[\"word\"].astype(str).str.strip().str.lower()\n",
        "    out = df[df[\"_word_norm\"].isin(forms)].drop(columns=[\"_word_norm\"])\n",
        "    return out.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e3e9769",
      "metadata": {
        "id": "6e3e9769"
      },
      "source": [
        "## How to use\n",
        "\n",
        "1. Set `root_or_underlying` to either:\n",
        "   - a single root form like `iti` / `sai` / `ko`, **or**\n",
        "   - an explicit sequence like `mo + ya` or `koN + ko` (case-insensitive).\n",
        "2. Set `pos` to either:\n",
        "   - `\"auto\"` (recommended if you only input a root), **or**\n",
        "   - a specific tag: `intrg pron`, `DPr`, `indfpro`, `expl`.\n",
        "3. Run the next cell to get candidate surface forms and (if the CSV is available) lexicon matches.\n",
        "\n",
        "Tip: For lexicalized contractions (e.g., `ma`), enter the full underlying sequence (`mo + ya`) because the contraction depends on both tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779927be",
      "metadata": {
        "id": "779927be",
        "outputId": "2d70f30a-f364-4baa-b3a4-67a1a6b10323"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>root word</th>\n",
              "      <th>surface</th>\n",
              "      <th>rule</th>\n",
              "      <th>rule_type</th>\n",
              "      <th>reversibility</th>\n",
              "      <th>weight</th>\n",
              "      <th>notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sain</td>\n",
              "      <td>ansain</td>\n",
              "      <td>INTRG_an_prefix_optional_add</td>\n",
              "      <td>morphophonemic</td>\n",
              "      <td>semi-reversible</td>\n",
              "      <td>0.55</td>\n",
              "      <td>Optional/historical an- prefix (semi-reversible).</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sain</td>\n",
              "      <td>sain</td>\n",
              "      <td>LEX_FALLBACK_identity</td>\n",
              "      <td>lexicalized</td>\n",
              "      <td>non-reversible</td>\n",
              "      <td>0.10</td>\n",
              "      <td>Identity fallback: keep underlying as surface ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  root word surface                          rule       rule_type  \\\n",
              "0      sain  ansain  INTRG_an_prefix_optional_add  morphophonemic   \n",
              "1      sain    sain         LEX_FALLBACK_identity     lexicalized   \n",
              "\n",
              "     reversibility  weight                                              notes  \n",
              "0  semi-reversible    0.55  Optional/historical an- prefix (semi-reversible).  \n",
              "1   non-reversible    0.10  Identity fallback: keep underlying as surface ...  "
            ]
          },
          "execution_count": 332,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- User inputs ---\n",
        "root_or_underlying = \"sain\"   # examples: \"sai\", \"iti\", \"mo + ya\", \"koN + ko\"\n",
        "pos = \"auto\"                # \"auto\" lets you input only a root and still get candidates\n",
        "\n",
        "# --- Generation knobs (keep constrained by default) ---\n",
        "allow_dpr_linker_suffix = False\n",
        "allow_productive_kon_prefixing = False  # set True if you want candidates like ko -> kongko / ikongko\n",
        "\n",
        "cands = generate_surface_candidates(\n",
        "    root_or_underlying,\n",
        "    pos,\n",
        "    allow_dpr_linker_suffix=allow_dpr_linker_suffix,\n",
        "    allow_productive_kon_prefixing=allow_productive_kon_prefixing,\n",
        "    max_results=25,\n",
        " )\n",
        "\n",
        "cand_df = pd.DataFrame(cands)\n",
        "\n",
        "# Rename columns for presentation\n",
        "rename_map = {\"underlying\": \"root word\"}\n",
        "cand_df = cand_df.rename(columns=rename_map)\n",
        "\n",
        "# Remove POS column from display (still used internally for generation)\n",
        "cand_df = cand_df.drop(columns=[c for c in [\"pos\", \"pos of root word\"] if c in cand_df.columns])\n",
        "\n",
        "# Reorder columns for readability\n",
        "preferred = [\"root word\", \"surface\", \"rule\", \"rule_type\", \"reversibility\", \"weight\", \"notes\",\n",
        "]\n",
        "ordered_cols = [c for c in preferred if c in cand_df.columns] + [c for c in cand_df.columns if c not in preferred]\n",
        "cand_df = cand_df[ordered_cols]\n",
        "\n",
        "cand_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395c33a2",
      "metadata": {
        "id": "395c33a2",
        "outputId": "f915ba8e-9e8e-4456-80e0-8c1929fba15e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 / 2 surface form(s) in the lexicon.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surface</th>\n",
              "      <th>in_lexicon</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ansain</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sain</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  surface  in_lexicon\n",
              "0  ansain        True\n",
              "1    sain        True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>part_of_speech</th>\n",
              "      <th>meaning_english</th>\n",
              "      <th>meaning_filipino</th>\n",
              "      <th>sample_bolinao</th>\n",
              "      <th>sample_english</th>\n",
              "      <th>upos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ansain</td>\n",
              "      <td>intrg pron</td>\n",
              "      <td>Identifies a specific grouping of things close to the hearer when preceded by \"no\".</td>\n",
              "      <td>Kung ano.</td>\n",
              "      <td>Ansain say lako' mo?</td>\n",
              "      <td>What are those which you are selling?</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sain</td>\n",
              "      <td>pr</td>\n",
              "      <td>That group or activity near you, the hearer or an activity that happened previously.</td>\n",
              "      <td>Mga iyan.</td>\n",
              "      <td>Sain tamo' a awit mo?</td>\n",
              "      <td>Is it only those things that you will take?</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     word part_of_speech  \\\n",
              "0  ansain     intrg pron   \n",
              "1    sain             pr   \n",
              "\n",
              "                                                                        meaning_english  \\\n",
              "0   Identifies a specific grouping of things close to the hearer when preceded by \"no\".   \n",
              "1  That group or activity near you, the hearer or an activity that happened previously.   \n",
              "\n",
              "  meaning_filipino         sample_bolinao  \\\n",
              "0        Kung ano.   Ansain say lako' mo?   \n",
              "1        Mga iyan.  Sain tamo' a awit mo?   \n",
              "\n",
              "                                sample_english  upos  \n",
              "0        What are those which you are selling?  PRON  \n",
              "1  Is it only those things that you will take?   ADP  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Verify surface candidates against the lexicon ---\n",
        "\n",
        "# Collect surface forms from the previous cell\n",
        "if \"cand_df\" in globals() and isinstance(cand_df, pd.DataFrame) and \"surface\" in cand_df.columns:\n",
        "    surface_forms = cand_df[\"surface\"].astype(str).tolist()\n",
        "elif \"cands\" in globals():\n",
        "    surface_forms = [str(d.get(\"surface\", \"\")) for d in cands]\n",
        "else:\n",
        "    surface_forms = []\n",
        "surface_forms = [s.strip() for s in surface_forms if str(s).strip()]\n",
        "surface_forms_unique = list(dict.fromkeys(surface_forms))\n",
        "\n",
        "# Make pandas display show full contents (no truncation) for this output only.\n",
        "_display_opts = {\n",
        "    \"display.max_colwidth\": None,\n",
        "    \"display.max_columns\": None,\n",
        "    \"display.max_rows\": None,\n",
        "    \"display.width\": 0,\n",
        "}\n",
        "\n",
        "if bolinao_final is None:\n",
        "    print(\"Lexicon CSV was not loaded (check LEXICON_PATH). Cannot verify surface forms.\")\n",
        "    verified_df = pd.DataFrame({\"surface\": surface_forms_unique, \"in_lexicon\": [False] * len(surface_forms_unique)})\n",
        "    with pd.option_context(*sum(_display_opts.items(), ())):\n",
        "        display(verified_df)\n",
        "else:\n",
        "    if \"word\" not in bolinao_final.columns:\n",
        "        raise ValueError(\"Lexicon DataFrame is missing required column: 'word'\")\n",
        "    lex = bolinao_final.copy()\n",
        "    lex[\"_word_norm\"] = lex[\"word\"].astype(str).str.strip().str.lower()\n",
        "    in_lex = set(lex[\"_word_norm\"].tolist())\n",
        "    verified_df = pd.DataFrame({\"surface\": surface_forms_unique})\n",
        "    verified_df[\"_surface_norm\"] = verified_df[\"surface\"].map(_norm)\n",
        "    verified_df[\"in_lexicon\"] = verified_df[\"_surface_norm\"].isin(in_lex)\n",
        "    verified_df = verified_df.drop(columns=[\"_surface_norm\"])\n",
        "    matched_rows = lex[lex[\"_word_norm\"].isin({_norm(s) for s in surface_forms_unique})].drop(columns=[\"_word_norm\"]).reset_index(drop=True)\n",
        "    print(f\"Found {int(verified_df['in_lexicon'].sum())} / {len(verified_df)} surface form(s) in the lexicon.\")\n",
        "    with pd.option_context(*sum(_display_opts.items(), ())):\n",
        "        display(verified_df)\n",
        "        if len(matched_rows) > 0:\n",
        "            display(matched_rows)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}