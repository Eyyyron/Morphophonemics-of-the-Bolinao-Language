{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ““ Undoing Assimilation in the Bolinao Lexicon\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Assimilation in Bolinao**\n",
        "\n",
        "Assimilation happens when affixes or clitics adjust their form to match the sounds they attach to. In Bolinao, it shows up in several places:\n",
        "\n",
        "### **1.1 Verb affixes (maN-, aN-, saN-)**\n",
        "- `man + bayo â†’ mambayo`\n",
        "- `man + ka + mati â†’ mangkamati`\n",
        "- `an + giling â†’ manggiling`\n",
        "- `saN- + kataâ€™gayan â†’ sangkataâ€™gayan`\n",
        "\n",
        "Rule: the **/n/** in maN-/aN-/saN- changes place of articulation to match the first consonant of the root.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.2 Pronoun prefixes (koN-, ikon-)**\n",
        "- `koN- + ta â†’ konta`\n",
        "- `ikon + ta â†’ ikonta`\n",
        "- `koN- + mi â†’ komi`\n",
        "\n",
        "Rule: the **/N/** assimilates to the first consonant of the pronoun root.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.3 Deictic pronouns (iti, isen, itaw)**\n",
        "- `mo + iti â†’ modti`\n",
        "- `mo + isen â†’ modsen`\n",
        "- `mo + itaw â†’ modtaw`\n",
        "\n",
        "Rule: encliticization turns the glottal + /i/ into **/d/**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.4 Linkers (a, nin)**\n",
        "- `Mangansyon ya a anak â†’ Mangansyon yay anak`\n",
        "- `Aripen nako nin Dios â†’ Aripen nakon Dios`\n",
        "\n",
        "Rule: linker changes shape to **-y** or **-n** after vowel-final words.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bNgFqJ9kfDKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Undoing Assimilation"
      ],
      "metadata": {
        "id": "AR0mV8Ie2c36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Exploration\n",
        "Weâ€™ll load the Bolinao lexicon CSV, inspect it, and prep for processing."
      ],
      "metadata": {
        "id": "gccE1zwvfPOX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkSCdGBle132",
        "outputId": "f9d0b78c-ddef-42b4-e172-d79d2acbf1b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     word part_of_speech                                    meaning_english  \\\n",
            "0    a'lo              n  Pestle, a rounded piece of wood about five inc...   \n",
            "1   a'nak              n  Referring to specific children individually, n...   \n",
            "2   a'nem              n                    Six, the number following five.   \n",
            "3   a'pat              n                                              Four.   \n",
            "4  a'rong              n                                              Nose.   \n",
            "\n",
            "  meaning_filipino                                     sample_bolinao  \\\n",
            "0            Halo.      Kustoy byat nansi a'lonman'ipambayo kon irik.   \n",
            "1        Mga anak.  Si Ligaya a kaka sa sarba konran syam nin a'na...   \n",
            "2            Anim.                         A'nem ray salay nan manok.   \n",
            "3            Apat.                Nagbakasyon ako nin a'pat nin awro.   \n",
            "4           Ilong.  Say a'rong ran Pilipino ket ambo' tuloy nin ma...   \n",
            "\n",
            "                                      sample_english  upos  \n",
            "0  Thepestlethat I am using to pound unhusked ric...  NOUN  \n",
            "1     Ligaya is the oldest of Gorio's nine children.  NOUN  \n",
            "2                          The chicken had six eggs.  NOUN  \n",
            "3                 I took a vacation for four months.  NOUN  \n",
            "4        The noses of Filipinos are not too pointed.  NOUN  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20307 entries, 0 to 20306\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   word              20307 non-null  object\n",
            " 1   part_of_speech    20307 non-null  object\n",
            " 2   meaning_english   20305 non-null  object\n",
            " 3   meaning_filipino  18378 non-null  object\n",
            " 4   sample_bolinao    18158 non-null  object\n",
            " 5   sample_english    18156 non-null  object\n",
            " 6   upos              20307 non-null  object\n",
            "dtypes: object(7)\n",
            "memory usage: 1.1+ MB\n",
            "None\n",
            "Unique words: 20307\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"bolinao_lexicon.csv\")\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(\"Unique words:\", df['word'].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Undoing Assimilation Rules\n",
        "\n",
        "Weâ€™ll create a function to generate possible root candidates for each assimilation type."
      ],
      "metadata": {
        "id": "wc2J-ME7fb9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_assimilation(word: str):\n",
        "    candidates = []\n",
        "\n",
        "    # --- Verb affixes (maN-, aN-, saN-) ---\n",
        "    if word.startswith(\"mamb\"):\n",
        "        candidates.append(\"b\" + word[4:])    # mambayo -> bayo\n",
        "    elif word.startswith(\"mamp\"):\n",
        "        candidates.append(\"p\" + word[4:])\n",
        "    elif word.startswith(\"mamm\"):\n",
        "        candidates.append(\"m\" + word[4:])\n",
        "    elif word.startswith(\"mang\"):\n",
        "        candidates.append(\"k\" + word[4:])\n",
        "    elif word.startswith(\"manng\"):\n",
        "        candidates.append(\"ng\" + word[5:])\n",
        "    elif word.startswith(\"man\"):\n",
        "        candidates.append(word[3:])          # fallback\n",
        "\n",
        "    if word.startswith(\"an\"):\n",
        "        candidates.append(word[2:])          # an+root\n",
        "    if word.startswith(\"san\"):\n",
        "        candidates.append(word[3:])          # san+root\n",
        "\n",
        "    # --- Pronoun prefixes (koN-, ikon-) ---\n",
        "    if word.startswith(\"kon\"):\n",
        "        candidates.append(word[3:])          # konta -> ta\n",
        "    elif word.startswith(\"kom\"):\n",
        "        candidates.append(\"mi\")              # komi -> mi\n",
        "    elif word.startswith(\"iko\"):\n",
        "        candidates.append(word[2:])          # ikonra -> ra\n",
        "\n",
        "    # --- Deictic pronouns ---\n",
        "    if word.startswith(\"modtaw\"):\n",
        "        candidates.append(\"itaw\")\n",
        "    elif word.startswith(\"modt\"):\n",
        "        candidates.append(\"iti\")             # modti -> iti\n",
        "    elif word.startswith(\"mods\"):\n",
        "        candidates.append(\"isen\")\n",
        "\n",
        "    # --- Linkers (tightened) ---\n",
        "    if len(word) <= 5:\n",
        "        if word.endswith(\"y\") and not word.endswith((\"ay\", \"oy\", \"uy\")):\n",
        "            candidates.append(word[:-1] + \" a\")  # yay -> ya a\n",
        "        if word.endswith(\"n\") and not word.endswith((\"an\", \"en\", \"on\", \"in\")):\n",
        "            candidates.append(word[:-1] + \" nin\")  # nakon -> nako nin\n",
        "\n",
        "    return list(set(candidates))\n"
      ],
      "metadata": {
        "id": "q_JPAzaoffI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Cross-Checking with the Lexicon\n",
        "\n",
        "Now weâ€™ll test each word, generate candidates, and see if they exist in the lexicon with the same meaning."
      ],
      "metadata": {
        "id": "7xbuAu2wfjaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confirmed = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    word = row['word']\n",
        "    meaning = row['meaning_english']\n",
        "    upos_assimilated = row['upos']\n",
        "    roots = undo_assimilation(word)\n",
        "\n",
        "    for r in roots:\n",
        "        match = df[df['word'].str.strip() == r.strip()]\n",
        "        if not match.empty:\n",
        "            record = {\n",
        "                \"assimilated\": word,\n",
        "                \"root_candidate\": r,\n",
        "                \"meaning_assimilated\": meaning,\n",
        "                \"meaning_root\": \"; \".join(match['meaning_english'].unique()),\n",
        "                \"upos_assimilated\": upos_assimilated,\n",
        "                \"upos_root\": \"; \".join(match['upos'].unique())\n",
        "            }\n",
        "            confirmed.append(record)\n",
        "\n",
        "confirmed_df = pd.DataFrame(confirmed)"
      ],
      "metadata": {
        "id": "m0Omm4BKfj9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Save Final Root Words\n",
        "\n",
        "Export confirmed pairs for further linguistic analysis."
      ],
      "metadata": {
        "id": "oSJHk0xcfpqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confirmed_df.to_csv(\"bolinao_root_words_assimilation.csv\", index=False)"
      ],
      "metadata": {
        "id": "QTOrgsl5foaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redoing Assimilation"
      ],
      "metadata": {
        "id": "b0EGSCdo2jym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    try:\n",
        "        df_lexicon = pd.read_csv(\"Bolinao_Lexicon.csv\", encoding='latin-1')\n",
        "    except UnicodeDecodeError:\n",
        "        df_lexicon = pd.read_csv(\"Bolinao_Lexicon.csv\", encoding='cp1252')\n",
        "    df_lexicon['word_clean'] = df_lexicon['word'].astype(str).str.lower().str.strip()\n",
        "    existing_words = set(df_lexicon['word_clean'].unique())\n",
        "    print(f\"Lexicon loaded: {len(existing_words)} unique words.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: bolinao_lexicon.csv not found.\")\n",
        "    existing_words = set()\n",
        "\n",
        "core_wordlist = [\n",
        "    {\"root\": \"gamet\", \"meaning\": \"hand\"}, {\"root\": \"wiri\", \"meaning\": \"left\"},\n",
        "    {\"root\": \"wanan\", \"meaning\": \"right\"}, {\"root\": \"bitih\", \"meaning\": \"leg/foot\"},\n",
        "    {\"root\": \"daan\", \"meaning\": \"road/path\"}, {\"root\": \"tangoy\", \"meaning\": \"to swim\"},\n",
        "    {\"root\": \"tapok\", \"meaning\": \"dust\"}, {\"root\": \"katat\", \"meaning\": \"skin\"},\n",
        "    {\"root\": \"gorot\", \"meaning\": \"back\"}, {\"root\": \"tyan\", \"meaning\": \"belly\"},\n",
        "    {\"root\": \"botol\", \"meaning\": \"bone\"}, {\"root\": \"agtay\", \"meaning\": \"liver\"},\n",
        "    {\"root\": \"soso\", \"meaning\": \"breast\"}, {\"root\": \"abaya\", \"meaning\": \"shoulder\"},\n",
        "    {\"root\": \"daya\", \"meaning\": \"blood\"}, {\"root\": \"olo\", \"meaning\": \"head\"},\n",
        "    {\"root\": \"leey\", \"meaning\": \"neck\"}, {\"root\": \"sabot\", \"meaning\": \"hair\"},\n",
        "    {\"root\": \"arong\", \"meaning\": \"nose\"}, {\"root\": \"angot\", \"meaning\": \"to sniff/smell\"},\n",
        "    {\"root\": \"bebey\", \"meaning\": \"mouth\"}, {\"root\": \"ngipin\", \"meaning\": \"tooth\"},\n",
        "    {\"root\": \"dila\", \"meaning\": \"tongue\"}, {\"root\": \"kalis\", \"meaning\": \"to laugh\"},\n",
        "    {\"root\": \"akis\", \"meaning\": \"to cry\"}, {\"root\": \"soka\", \"meaning\": \"to vomit\"},\n",
        "    {\"root\": \"kan\", \"meaning\": \"to eat\"}, {\"root\": \"inom\", \"meaning\": \"to drink\"},\n",
        "    {\"root\": \"kayat\", \"meaning\": \"to bite\"}, {\"root\": \"sepsep\", \"meaning\": \"to suck\"},\n",
        "    {\"root\": \"toly\", \"meaning\": \"ear\"}, {\"root\": \"ingar\", \"meaning\": \"to hear\"},\n",
        "    {\"root\": \"mata\", \"meaning\": \"eye\"}, {\"root\": \"kit\", \"meaning\": \"to see\"},\n",
        "    {\"root\": \"elek\", \"meaning\": \"to sleep\"}, {\"root\": \"taynep\", \"meaning\": \"to dream\"},\n",
        "    {\"root\": \"tekre\", \"meaning\": \"to sit\"}, {\"root\": \"ideng\", \"meaning\": \"to stand\"},\n",
        "    {\"root\": \"lalaki\", \"meaning\": \"man/male\"}, {\"root\": \"babayi\", \"meaning\": \"woman/female\"},\n",
        "    {\"root\": \"anak\", \"meaning\": \"child\"}, {\"root\": \"ahawa\", \"meaning\": \"spouse\"},\n",
        "    {\"root\": \"ina\", \"meaning\": \"mother\"}, {\"root\": \"tatay\", \"meaning\": \"father\"},\n",
        "    {\"root\": \"bali\", \"meaning\": \"house\"}, {\"root\": \"atep\", \"meaning\": \"roof\"},\n",
        "    {\"root\": \"ngaran\", \"meaning\": \"name\"}, {\"root\": \"robir\", \"meaning\": \"rope\"},\n",
        "    {\"root\": \"tayi\", \"meaning\": \"to sew\"}, {\"root\": \"kadayem\", \"meaning\": \"needle\"},\n",
        "    {\"root\": \"takaw\", \"meaning\": \"to steal\"}, {\"root\": \"pati\", \"meaning\": \"to kill\"},\n",
        "    {\"root\": \"tadem\", \"meaning\": \"sharp\"}, {\"root\": \"obra\", \"meaning\": \"to work\"},\n",
        "    {\"root\": \"tanem\", \"meaning\": \"to plant\"}, {\"root\": \"pili\", \"meaning\": \"to choose\"},\n",
        "    {\"root\": \"pespes\", \"meaning\": \"to squeeze\"}, {\"root\": \"kotkot\", \"meaning\": \"to dig\"},\n",
        "    {\"root\": \"haliw\", \"meaning\": \"to buy\"}, {\"root\": \"bantak\", \"meaning\": \"to throw\"},\n",
        "    {\"root\": \"aso\", \"meaning\": \"dog\"}, {\"root\": \"manok\", \"meaning\": \"bird/chicken\"},\n",
        "    {\"root\": \"salay\", \"meaning\": \"egg\"}, {\"root\": \"pakpak\", \"meaning\": \"wing\"},\n",
        "    {\"root\": \"lompad\", \"meaning\": \"to fly\"}, {\"root\": \"ikoy\", \"meaning\": \"tail\"},\n",
        "    {\"root\": \"olay\", \"meaning\": \"snake\"}, {\"root\": \"bolati\", \"meaning\": \"worm\"},\n",
        "    {\"root\": \"gigang\", \"meaning\": \"spider\"}, {\"root\": \"kona\", \"meaning\": \"fish\"},\n",
        "    {\"root\": \"yamot\", \"meaning\": \"root\"}, {\"root\": \"bonga\", \"meaning\": \"fruit\"},\n",
        "    {\"root\": \"bato\", \"meaning\": \"stone\"}, {\"root\": \"boyangin\", \"meaning\": \"sand\"},\n",
        "    {\"root\": \"ranom\", \"meaning\": \"water\"}, {\"root\": \"asin\", \"meaning\": \"salt\"},\n",
        "    {\"root\": \"langit\", \"meaning\": \"sky\"}, {\"root\": \"bulan\", \"meaning\": \"moon\"},\n",
        "    {\"root\": \"bitoen\", \"meaning\": \"star\"}, {\"root\": \"gonem\", \"meaning\": \"cloud\"},\n",
        "    {\"root\": \"rapeg\", \"meaning\": \"rain\"}, {\"root\": \"kodor\", \"meaning\": \"thunder\"},\n",
        "    {\"root\": \"kimat\", \"meaning\": \"lightning\"}, {\"root\": \"emot\", \"meaning\": \"warm\"},\n",
        "    {\"root\": \"rayep\", \"meaning\": \"cold\"}, {\"root\": \"albet\", \"meaning\": \"wet\"},\n",
        "    {\"root\": \"byat\", \"meaning\": \"heavy\"}\n",
        "]\n",
        "\n",
        "def apply_assimilation(root):\n",
        "    candidates = []\n",
        "    r = root.lower().strip()\n",
        "    if not r: return []\n",
        "\n",
        "    # Rule 1: Bilabials (b, p) -> mamb- / mam-\n",
        "    if r.startswith(('b', 'p')):\n",
        "        candidates.append(f\"mam{r[1:]}\")   # Total Assimilation (bayo -> mamayo)\n",
        "        if r.startswith('b'):\n",
        "            candidates.append(f\"mamb{r[1:]}\") # Partial Assimilation (bayo -> mambayo)\n",
        "\n",
        "    # Rule 2: Alveolars (d, s, t) -> man-\n",
        "    elif r.startswith(('d', 's', 't')):\n",
        "        candidates.append(f\"man{r[1:]}\")   # Total Assimilation (takaw -> manakaw)\n",
        "\n",
        "    # Rule 3: Velar (k) -> mang- (k dropped)\n",
        "    elif r.startswith('k'):\n",
        "        candidates.append(f\"mang{r[1:]}\")  # Total Assimilation (kimit -> mangimit)\n",
        "\n",
        "    # Rule 4: Glottal / Vowel -> mang-\n",
        "    elif r[0] in 'aeiou':\n",
        "        candidates.append(f\"mang{r}\")      # Standard (anak -> manganak)\n",
        "        candidates.append(f\"mang-{r}\")     # Hyphenated variation\n",
        "\n",
        "    # Rule 5: Other Consonants (l, r, w, y, g, h, n, m) -> mang- (No change to root)\n",
        "    else:\n",
        "        candidates.append(f\"mang{r}\")      # (gamet -> manggamet)\n",
        "\n",
        "    return list(set(candidates))\n",
        "\n",
        "results = []\n",
        "\n",
        "for item in core_wordlist:\n",
        "    root = item['root']\n",
        "    root_meaning = item['meaning']\n",
        "\n",
        "    generated_forms = apply_assimilation(root)\n",
        "\n",
        "    for gen_word in generated_forms:\n",
        "        status = \"Verified in Lexicon\" if gen_word in existing_words else \"New Candidate\"\n",
        "\n",
        "        # Determine Meaning\n",
        "        if status == \"Verified in Lexicon\":\n",
        "            match = df_lexicon[df_lexicon['word_clean'] == gen_word]\n",
        "            if not match.empty:\n",
        "                final_meaning = match.iloc[0]['meaning_english']\n",
        "            else:\n",
        "                final_meaning = \"Found (No definition)\"\n",
        "        else:\n",
        "            # Generate Theoretical Meaning\n",
        "            clean_root_meaning = root_meaning\n",
        "            if clean_root_meaning.lower().startswith(\"to \"):\n",
        "                clean_root_meaning = clean_root_meaning[3:]\n",
        "\n",
        "            final_meaning = f\"[Predicted] To perform the action: {clean_root_meaning}\"\n",
        "\n",
        "        results.append({\n",
        "            \"Core Root\": root,\n",
        "            \"Root Meaning\": root_meaning,\n",
        "            \"Generated Word\": gen_word,\n",
        "            \"Status\": status,\n",
        "            \"Final Meaning\": final_meaning\n",
        "        })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "verified = df_results[df_results['Status'] == \"Verified in Lexicon\"]\n",
        "theoretical = df_results[df_results['Status'] == \"New Candidate\"]\n",
        "\n",
        "print(f\"--- SUMMARY ---\")\n",
        "print(f\"Core Roots Tested: {len(core_wordlist)}\")\n",
        "print(f\"Verified Assimilated Forms: {len(verified)}\")\n",
        "print(f\"New Candidates Generated: {len(theoretical)}\")\n",
        "\n",
        "print(\"\\n--- SAMPLE VERIFIED MATCHES (Table 6) ---\")\n",
        "print(verified[['Core Root', 'Generated Word', 'Final Meaning']].head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\n--- SAMPLE NEW CANDIDATES (Table 7) ---\")\n",
        "print(theoretical[['Core Root', 'Generated Word', 'Final Meaning']].head(10).to_string(index=False))\n",
        "df_results.to_csv(\"bolinao_core2_assimilation.csv\", index=False)"
      ],
      "metadata": {
        "id": "PA43IKHt2nhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}